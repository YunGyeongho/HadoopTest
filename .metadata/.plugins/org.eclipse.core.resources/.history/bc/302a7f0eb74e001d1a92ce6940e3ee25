# -*- coding:utf-8 -*-
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from konlpy.tag._okt import Okt

class AiAdFilter():
    
    def __init__(self, CountVectorizer(), MultinomialNB(), Okt()):
        self.Count
        self.mnbAI = mnbAI
        self.okt = okt

    def trainAI():
        global cv, mnbAI
        trainData = []
        label = []
    
        con = MongoClient("192.168.0.100") # MongoDB서버에 연결
        db = con.xe # DB에 연결 : con.db명
    
        blogData = db.haheeho.find()
    
        for bd in blogData:
            trainData.append(bd["cd_content"])
            label.append(bd["cd_label"])
            
        con.close()
    
            # cv.fit_transform 
            # 들어온 데이터를 이용해서 중복되지않은 단어를 기준으로 
        cvResult = cv.fit_transform(trainData)
            # cv.fit_transform을 기준으로 trainData를 2차원배열 형식으로 바꿔줌. 
        cvResult = cvResult.toarray() 
            
            
        mnb = MultinomialNB()
            # cvResult와 label을 이용해 AI학습
        mnbAI = mnb.fit(cvResult, label)
    
    def AIpredict(url):
        global cv, mnbAI
            
            # 받아온 url로 블로그 내용 가져와서
        totalText = getMainContainer(getMainFrameSrc(url))
            # trainData와 마찬가지로 블로그 내용 정규화 및 
        totalText = o.normalize(totalText)
            # 기본형으로 바꾸고 (기본형, pos) 튜플로 나옴
        totalText = o.pos(totalText, stem=True)
        
            # word만 빼와서 스트링으로 붙여서 words리스트에 append함
            words=[]
        for word, pos in totalText:
            words.append("%s " % word)
        
            # words 리스트에 있는 데이터들을 ''.join(words)를 써서 string 한 줄로 빼기 
        sentence = [''.join(words)]
    
            # trainAI 함수에서 fit_transform에 세워둔 기준에 맞춰 전처리
        sentenceCvResult = cv.transform(sentence)
        sentenceCvResult = sentenceCvResult.toarray()
            
        result = mnbAI.predict(sentenceCvResult) # 예측
        return result
    
    # AI를 훈련을 미리 시키고
    trainAI()
    
    # 이건 임시 url이고 실전에선 java에서 받아온 url
    url =  "https://blog.naver.com/songsb4125/222844143711"
    
    # AI에게 java에서 받아온 url넣어주고 광고인지 아닌지 판별해서 뱉어주는 함수
    result = AIpredict(url)
    print(result)